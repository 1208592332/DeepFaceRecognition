{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from nn.Network import getModel,  summaryBuilder\n",
    "\n",
    "from nn.LoadParams import layer_name, convShape, getWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "(7, 7, 3, 64)\n",
      "(64,)\n",
      "bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv2\n",
      "(1, 1, 64, 64)\n",
      "(64,)\n",
      "bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv3\n",
      "(3, 3, 64, 192)\n",
      "(192,)\n",
      "bn3\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_3a_1x1_conv\n",
      "(1, 1, 192, 64)\n",
      "(64,)\n",
      "inception_3a_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3a_5x5_conv1\n",
      "(1, 1, 192, 16)\n",
      "(16,)\n",
      "inception_3a_5x5_conv2\n",
      "(5, 5, 16, 32)\n",
      "(32,)\n",
      "inception_3a_5x5_bn1\n",
      "(16,) (16,) (16,) (16,)\n",
      "inception_3a_5x5_bn2\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3a_3x3_conv1\n",
      "(1, 1, 192, 96)\n",
      "(96,)\n",
      "inception_3a_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3a_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3a_pool_conv\n",
      "(1, 1, 192, 32)\n",
      "(32,)\n",
      "inception_3a_pool_bn\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_1x1_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_3x3_conv1\n",
      "(1, 1, 256, 96)\n",
      "(96,)\n",
      "inception_3b_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3b_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3b_5x5_conv1\n",
      "(1, 1, 256, 32)\n",
      "(32,)\n",
      "inception_3b_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3b_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_pool_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_pool_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3c_3x3_conv1\n",
      "(1, 1, 320, 128)\n",
      "(128,)\n",
      "inception_3c_3x3_conv2\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "inception_3c_3x3_bn1\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3c_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_3c_5x5_conv1\n",
      "(1, 1, 320, 32)\n",
      "(32,)\n",
      "inception_3c_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3c_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3c_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_1x1_conv\n",
      "(1, 1, 640, 256)\n",
      "(256,)\n",
      "inception_4a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4a_3x3_conv1\n",
      "(1, 1, 640, 96)\n",
      "(96,)\n",
      "inception_4a_3x3_conv2\n",
      "(3, 3, 96, 192)\n",
      "(192,)\n",
      "inception_4a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_4a_3x3_bn2\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_4a_5x5_conv1\n",
      "(1, 1, 640, 32)\n",
      "(32,)\n",
      "inception_4a_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_4a_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_4a_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_pool_conv\n",
      "(1, 1, 640, 128)\n",
      "(128,)\n",
      "inception_4a_pool_bn\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_4e_3x3_conv1\n",
      "(1, 1, 640, 160)\n",
      "(160,)\n",
      "inception_4e_3x3_conv2\n",
      "(3, 3, 160, 256)\n",
      "(256,)\n",
      "inception_4e_3x3_bn1\n",
      "(160,) (160,) (160,) (160,)\n",
      "inception_4e_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4e_5x5_conv1\n",
      "(1, 1, 640, 64)\n",
      "(64,)\n",
      "inception_4e_5x5_conv2\n",
      "(5, 5, 64, 128)\n",
      "(128,)\n",
      "inception_4e_5x5_bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4e_5x5_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_5a_1x1_conv\n",
      "(1, 1, 1024, 256)\n",
      "(256,)\n",
      "inception_5a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5a_3x3_conv1\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5a_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5a_pool_conv\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_1x1_conv\n",
      "(1, 1, 736, 256)\n",
      "(256,)\n",
      "inception_5b_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5b_3x3_conv1\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5b_pool_conv\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n"
     ]
    }
   ],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def initNetwork(weightDict):\n",
    "    reset_graph()\n",
    "    tensorDict = getModel([96,96,3], params=weightDict)\n",
    "    return tensorDict\n",
    "\n",
    "def loadData(dataPath):\n",
    "    with open(dataPath, \"rb\") as p:\n",
    "        data = pickle.load(p)\n",
    "        dataX = data['dataX']\n",
    "        dataY = data['dataY']\n",
    "        labelDict = data['labelDict']\n",
    "    return dataX, dataY, labelDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN NETWORK - CREATE ENCODINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 32)\n",
      "inception_pool Chain 4:  (?, 12, 12, 32)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 64)\n",
      "inception_pool Chain 4:  (?, 12, 12, 64)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 256)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "pool_pad Chain 4:  (?, 6, 6, 320)\n",
      "inception3c:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 192)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "inception_pool Chain 4:  (?, 6, 6, 128)\n",
      "inception_1x1: Chain 1:  (?, 6, 6, 256)\n",
      "inception4a:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 256)\n",
      "inception_5x5 Chain 3:  (?, 3, 3, 128)\n",
      "pool_pad Chain 4:  (?, 3, 3, 640)\n",
      "inception4e:  (?, 3, 3, 1024)\n",
      "Inside Inception module1:  (?, 3, 3, 1024)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5a:  (?, 3, 3, 736)\n",
      "Inside Inception module1:  (?, 3, 3, 736)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5b:  (?, 3, 3, 736)\n",
      "X after FC pool:  (?, 1, 1, 736)\n",
      "X after X Flattened:  (?, 736)\n",
      "X after FC Matmul:  (?, 128)\n",
      "(114, 128)\n"
     ]
    }
   ],
   "source": [
    "summaryOutputPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/summary\"\n",
    "\n",
    "dataPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/full_data_ndarr.pickle\"\n",
    "tensorDict = initNetwork(moduleWeightDict)\n",
    "dataX, dataY, labelDict = loadData(dataPath)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    mergedSummary, writer = summaryBuilder(sess, summaryOutputPath)\n",
    "    dataNorm = dataX/255\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:dataNorm})\n",
    "    writer = tf.summary.FileWriter(summaryOutputPath, sess.graph)\n",
    "    writer.close()\n",
    "    print (x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Previous Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n",
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 32)\n",
      "inception_pool Chain 4:  (?, 12, 12, 32)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 64)\n",
      "inception_pool Chain 4:  (?, 12, 12, 64)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 256)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "pool_pad Chain 4:  (?, 6, 6, 320)\n",
      "inception3c:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 192)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "inception_pool Chain 4:  (?, 6, 6, 128)\n",
      "inception_1x1: Chain 1:  (?, 6, 6, 256)\n",
      "inception4a:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 256)\n",
      "inception_5x5 Chain 3:  (?, 3, 3, 128)\n",
      "pool_pad Chain 4:  (?, 3, 3, 640)\n",
      "inception4e:  (?, 3, 3, 1024)\n",
      "Inside Inception module1:  (?, 3, 3, 1024)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5a:  (?, 3, 3, 736)\n",
      "Inside Inception module1:  (?, 3, 3, 736)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5b:  (?, 3, 3, 736)\n",
      "X after FC pool:  (?, 1, 1, 736)\n",
      "X after X Flattened:  (?, 736)\n",
      "X after FC Matmul:  (?, 128)\n",
      "(1, 96, 96, 3)\n",
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "picPath = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceRecognition/camera_0.jpg'\n",
    "\n",
    "from scipy import misc\n",
    "from nn.Network import getModel\n",
    "\n",
    "# Read an Image\n",
    "def readImage(imagePath):\n",
    "    '''\n",
    "        The input data is is in the shape of [nh, nw, nc], convert it to [nc, nh, nw]\n",
    "    '''\n",
    "    image = misc.imread(picPath)\n",
    "    \n",
    "    img = np.around(image/255.0, decimals=12) #(2,0,1) = [nc, nh, nw]\n",
    "    print (image.shape)\n",
    "    return img\n",
    "\n",
    "img = readImage(picPath)\n",
    "\n",
    "# Reset previously built graph\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# Get the inception Model\n",
    "reset_graph()\n",
    "tensorDict = getModel(img.shape, params=moduleWeightDict)#(img, conv1_w, conv1_b, s=2, pad='SAME',  scope_name='conv1', isTrainable=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print (img.shape)\n",
    "    img1 = np.reshape(img, (1,img.shape[0], img.shape[1], img.shape[2]))\n",
    "    print (img1.shape)\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:img1})\n",
    "    print (x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ROUGH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FROM THE FACE NET PAPER\n",
    "\n",
    "Choosing triplets (anchor, positive and negative) randomly then the loss function constraint is \n",
    "easily satisfied and we may end up learning a particular type of image and make our mode prone to \n",
    "overfitting the data.\n",
    "\n",
    "So we choose triplets that are hard to train on such that select an anchor compare to all pasotive\n",
    "examples and select the one whose encoding is furthest form the anchor, this is called Hardest Positive. \n",
    "Similarly while selecting a negative image select the image whose encoding is the closest to the \n",
    "achor image, this is called as selecting hardest negative.\n",
    "\n",
    "In practise it is often preferable to we use all anchor- positive pairs in a mini-batch \n",
    "while still selecting the hard negatives.\n",
    "\n",
    "* To Do's\n",
    "   * Extract faces out of the images using Haar Cascades\n",
    "   * Create encoding for each images and test if the encoding are by themself able to distinguish between \n",
    "     difrerent people\n",
    "   * Then check on Fine tuning the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/opt/concourse/volumes/live/65bc8d8a-a7db-4b09-4d80-394825911c99/volume/opencv_1512681450376/work/modules/imgproc/src/color.cpp:11016: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-75e40f6548d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimagePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/original/1.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimgRGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# gray = cv2.cvtColor(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /opt/concourse/volumes/live/65bc8d8a-a7db-4b09-4d80-394825911c99/volume/opencv_1512681450376/work/modules/imgproc/src/color.cpp:11016: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "imagePath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/original/1.png\"\n",
    "img = cv2.imread(imagePath)\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print (img)\n",
    "# gray = cv2.cvtColor(img)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/jenkins/miniconda/1/x64/conda-bld/conda_1486587097465/work/opencv-3.1.0/modules/highgui/src/window.cpp:281: error: (-215) size.width>0 && size.height>0 in function imshow\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60ac6371abda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/jenkins/miniconda/1/x64/conda-bld/conda_1486587097465/work/opencv-3.1.0/modules/highgui/src/window.cpp:281: error: (-215) size.width>0 && size.height>0 in function imshow\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = np.array([[1,2],[4,2],[1,1],[6,2],[4,3],[8,4],[1,2]])\n",
    "a_tf = tf.cast(a, dtype=tf.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-e21d2aa8ac28>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-e21d2aa8ac28>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    with open tf.Session() as sess:\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer)\n",
    "    b = tf.nn.l2_normalize(a_tf, dim=1, epsilon=1e-5)\n",
    "    print (b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
