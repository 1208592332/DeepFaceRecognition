{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "\n",
    "from DataPrep.data_io import DataFormatter\n",
    "from nn.network import getModel,  summaryBuilder\n",
    "\n",
    "from nn.load_params import layer_name, convShape, getWeights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "(7, 7, 3, 64)\n",
      "(64,)\n",
      "bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv2\n",
      "(1, 1, 64, 64)\n",
      "(64,)\n",
      "bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv3\n",
      "(3, 3, 64, 192)\n",
      "(192,)\n",
      "bn3\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_3a_1x1_conv\n",
      "(1, 1, 192, 64)\n",
      "(64,)\n",
      "inception_3a_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3a_5x5_conv1\n",
      "(1, 1, 192, 16)\n",
      "(16,)\n",
      "inception_3a_5x5_conv2\n",
      "(5, 5, 16, 32)\n",
      "(32,)\n",
      "inception_3a_5x5_bn1\n",
      "(16,) (16,) (16,) (16,)\n",
      "inception_3a_5x5_bn2\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3a_3x3_conv1\n",
      "(1, 1, 192, 96)\n",
      "(96,)\n",
      "inception_3a_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3a_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3a_pool_conv\n",
      "(1, 1, 192, 32)\n",
      "(32,)\n",
      "inception_3a_pool_bn\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_1x1_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_3x3_conv1\n",
      "(1, 1, 256, 96)\n",
      "(96,)\n",
      "inception_3b_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3b_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3b_5x5_conv1\n",
      "(1, 1, 256, 32)\n",
      "(32,)\n",
      "inception_3b_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3b_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_pool_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_pool_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3c_3x3_conv1\n",
      "(1, 1, 320, 128)\n",
      "(128,)\n",
      "inception_3c_3x3_conv2\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "inception_3c_3x3_bn1\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3c_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_3c_5x5_conv1\n",
      "(1, 1, 320, 32)\n",
      "(32,)\n",
      "inception_3c_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3c_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3c_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_1x1_conv\n",
      "(1, 1, 640, 256)\n",
      "(256,)\n",
      "inception_4a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4a_3x3_conv1\n",
      "(1, 1, 640, 96)\n",
      "(96,)\n",
      "inception_4a_3x3_conv2\n",
      "(3, 3, 96, 192)\n",
      "(192,)\n",
      "inception_4a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_4a_3x3_bn2\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_4a_5x5_conv1\n",
      "(1, 1, 640, 32)\n",
      "(32,)\n",
      "inception_4a_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_4a_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_4a_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_pool_conv\n",
      "(1, 1, 640, 128)\n",
      "(128,)\n",
      "inception_4a_pool_bn\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_4e_3x3_conv1\n",
      "(1, 1, 640, 160)\n",
      "(160,)\n",
      "inception_4e_3x3_conv2\n",
      "(3, 3, 160, 256)\n",
      "(256,)\n",
      "inception_4e_3x3_bn1\n",
      "(160,) (160,) (160,) (160,)\n",
      "inception_4e_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4e_5x5_conv1\n",
      "(1, 1, 640, 64)\n",
      "(64,)\n",
      "inception_4e_5x5_conv2\n",
      "(5, 5, 64, 128)\n",
      "(128,)\n",
      "inception_4e_5x5_bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4e_5x5_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_5a_1x1_conv\n",
      "(1, 1, 1024, 256)\n",
      "(256,)\n",
      "inception_5a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5a_3x3_conv1\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5a_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5a_pool_conv\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_1x1_conv\n",
      "(1, 1, 736, 256)\n",
      "(256,)\n",
      "inception_5b_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5b_3x3_conv1\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5b_pool_conv\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n"
     ]
    }
   ],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def initNetwork(weightDict):\n",
    "    reset_graph()\n",
    "    tensorDict = getModel([96,96,3], params=weightDict)\n",
    "    return tensorDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE ENCODINGS FOR VERIFICATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 32)\n",
      "inception_pool Chain 4:  (?, 12, 12, 32)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 64)\n",
      "inception_pool Chain 4:  (?, 12, 12, 64)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 256)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "pool_pad Chain 4:  (?, 6, 6, 320)\n",
      "inception3c:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 192)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "inception_pool Chain 4:  (?, 6, 6, 128)\n",
      "inception_1x1: Chain 1:  (?, 6, 6, 256)\n",
      "inception4a:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 256)\n",
      "inception_5x5 Chain 3:  (?, 3, 3, 128)\n",
      "pool_pad Chain 4:  (?, 3, 3, 640)\n",
      "inception4e:  (?, 3, 3, 1024)\n",
      "Inside Inception module1:  (?, 3, 3, 1024)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5a:  (?, 3, 3, 736)\n",
      "Inside Inception module1:  (?, 3, 3, 736)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5b:  (?, 3, 3, 736)\n",
      "X after FC pool:  (?, 1, 1, 736)\n",
      "X after X Flattened:  (?, 736)\n",
      "X after FC Matmul:  (?, 128)\n",
      "The shape of input data (X) is:  3\n",
      "The shape of input data (Y) is:  3\n",
      "Unique labels in dataY is:  [0 1 2]\n",
      "Label dict:  {'0': 'sam', '2': 'raj', '1': 'soham'}\n",
      "The shape of input data (X) is:  1\n",
      "The shape of input data (Y) is:  3\n",
      "Unique labels in dataY is:  [0 1 2]\n",
      "Label dict:  {'0': 'sam', '2': 'raj', '1': 'soham'}\n"
     ]
    }
   ],
   "source": [
    "dataPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models\"\n",
    "encodingPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/encodings/\"\n",
    "verification_data = 'verification_imgarr.pickle'\n",
    "verification_encoding_file = 'verification_img_encodings.pickle'\n",
    "\n",
    "tensorDict = initNetwork(moduleWeightDict)\n",
    "dataX, dataY, labelDict = DataFormatter.getPickleFile(dataPath, verification_data, getStats=True)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    dataNorm = dataX/255\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:dataNorm})\n",
    "    \n",
    "    DataFormatter.dumpPickleFile(x, dataY, labelDict=labelDict, folderPath=encodingPath,\n",
    "                                 picklefileName=verification_encoding_file, getStats=True)\n",
    "#     print (x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE ENCODINGS FOR TRAINING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 32)\n",
      "inception_pool Chain 4:  (?, 12, 12, 32)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 64)\n",
      "inception_pool Chain 4:  (?, 12, 12, 64)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 256)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "pool_pad Chain 4:  (?, 6, 6, 320)\n",
      "inception3c:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 192)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "inception_pool Chain 4:  (?, 6, 6, 128)\n",
      "inception_1x1: Chain 1:  (?, 6, 6, 256)\n",
      "inception4a:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 256)\n",
      "inception_5x5 Chain 3:  (?, 3, 3, 128)\n",
      "pool_pad Chain 4:  (?, 3, 3, 640)\n",
      "inception4e:  (?, 3, 3, 1024)\n",
      "Inside Inception module1:  (?, 3, 3, 1024)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5a:  (?, 3, 3, 736)\n",
      "Inside Inception module1:  (?, 3, 3, 736)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5b:  (?, 3, 3, 736)\n",
      "X after FC pool:  (?, 1, 1, 736)\n",
      "X after X Flattened:  (?, 736)\n",
      "X after FC Matmul:  (?, 128)\n",
      "(90, 96, 96, 3) (90, 1)\n"
     ]
    }
   ],
   "source": [
    "summaryOutputPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/summary\"\n",
    "dataPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models\"\n",
    "encodingPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/encodings/\"\n",
    "training_data = 'training_imgarr.pickle'\n",
    "training_encoding_file = 'training_img_encodings.pickle'\n",
    "\n",
    "tensorDict = initNetwork(moduleWeightDict)\n",
    "dataX, dataY, labelDict = DataFormatter.getPickleFile(dataPath, training_data)\n",
    "print(dataX.shape, dataY.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    mergedSummary, writer = summaryBuilder(sess, summaryOutputPath)\n",
    "    dataNorm = dataX/255\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:dataNorm})\n",
    "    writer = tf.summary.FileWriter(summaryOutputPath, sess.graph)\n",
    "    writer.close()\n",
    "    DataFormatter.dumpPickleFile(x, dataY, labelDict=labelDict, folderPath=encodingPath,\n",
    "                                 picklefileName=training_encoding_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Find the most closest Picture using only encodings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-37ffac69bd53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m verX, verY, verLabelDict = DataFormatter.getPickleFile(training_encoding_path, \n\u001b[1;32m      7\u001b[0m                                                        'verification_img_encodings.pickle')\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "training_encoding_path = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/encodings\"\n",
    "verification_encoding_path = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/encodings\"    \n",
    " \n",
    "trainX, trainY, trainLabelDict = DataFormatter.getPickleFile(training_encoding_path, \n",
    "                                                             'training_img_encodings.pickle')\n",
    "verX, verY, verLabelDict = DataFormatter.getPickleFile(training_encoding_path, \n",
    "                                                       'verification_img_encodings.pickle')\n",
    "print (trainX.shape, trainY.shape) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'ritu',\n",
       " '1': 'sam',\n",
       " '2': 'jetha',\n",
       " '3': 'pradyot',\n",
       " '4': 'soham',\n",
       " '5': 'chetan',\n",
       " '6': 'navneet',\n",
       " '7': 'raj'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLabelDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "TrainingData:  ritu\n",
      "[ 0.12038152 -0.014854   -0.03519159  0.0298566   0.07734559  0.08589125\n",
      "  0.18633814  0.0638134  -0.0186901  -0.1612308  -0.05174623  0.03851372\n",
      "  0.10089715 -0.1247714  -0.04237328 -0.09391901 -0.03459681 -0.00593167\n",
      " -0.16921839  0.15582454  0.08683969 -0.05168987  0.03058793  0.14142306\n",
      " -0.09239607 -0.24168749 -0.1542576  -0.14177261 -0.06083471  0.10674828\n",
      " -0.03752242 -0.02685932 -0.07723442  0.10103063 -0.00530118 -0.06769846\n",
      "  0.01051617  0.03453248 -0.18141574 -0.03448711  0.03758086 -0.02653159\n",
      "  0.01966289  0.02551536 -0.0915257  -0.01531907  0.02790756  0.02061272\n",
      " -0.04679441  0.18818331 -0.02952771 -0.21056528  0.01529649  0.01914737\n",
      "  0.04706831 -0.01424129 -0.08025786  0.09960169 -0.01191272 -0.01406639\n",
      " -0.00418146  0.11671023  0.04186565 -0.15259305  0.02494658 -0.11609522\n",
      "  0.0165531  -0.13106598 -0.18608896 -0.15082873 -0.02143332 -0.00638953\n",
      "  0.01496287  0.0348852  -0.06435134  0.09323955  0.07846075 -0.05588973\n",
      "  0.08066323  0.08887311 -0.00463023 -0.12225921 -0.11061607 -0.0250785\n",
      " -0.00894109  0.0941711  -0.01608691 -0.05430492 -0.06249554  0.0780205\n",
      "  0.10830531 -0.16787198 -0.02505144 -0.05405352 -0.06611424  0.03152025\n",
      " -0.02065098 -0.03189547  0.00557692  0.08124664  0.04427531  0.02160115\n",
      " -0.0292548   0.10479116 -0.10423094 -0.01833679 -0.05547468  0.1283377\n",
      " -0.09577047  0.05435596  0.04627135  0.03494681 -0.02343366 -0.01773294\n",
      " -0.12348083  0.12252201 -0.02220407  0.09346538 -0.07910539  0.12642704\n",
      " -0.12747175  0.01587924 -0.08084231  0.10657007 -0.00756407 -0.01159192\n",
      " -0.13208576 -0.17606623]\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/numpy/linalg/linalg.py:2168: RuntimeWarning: invalid value encountered in sqrt\n",
      "  ret = sqrt(sqnorm)\n"
     ]
    }
   ],
   "source": [
    "for t_encoding, t_label in zip(trainX, trainY):\n",
    "    print (t_label)\n",
    "    print ('TrainingData: ', trainLabelDict[str(t_label[0])])\n",
    "    print (t_encoding)\n",
    "    distArr = []\n",
    "    labelArr = []\n",
    "    for v_encoding, v_label in zip(verX, verY):\n",
    "        dist = np.linalg.norm(t_encoding - v_encoding)\n",
    "        distArr.append(dist)\n",
    "        labelArr.append(trainLabelDict[str(v_label[0])])\n",
    "#         print('distance is: 'dist)\n",
    "#         print (trainLabelDict[str(v_label[0])])\n",
    "    print(np.min(distArr))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Previous Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n",
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 32)\n",
      "inception_pool Chain 4:  (?, 12, 12, 32)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "inception_3x3 Chain 2:  (?, 12, 12, 128)\n",
      "inception_5x5 Chain 3:  (?, 12, 12, 64)\n",
      "inception_pool Chain 4:  (?, 12, 12, 64)\n",
      "inception_1x1: Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 256)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "pool_pad Chain 4:  (?, 6, 6, 320)\n",
      "inception3c:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 6, 6, 192)\n",
      "inception_5x5 Chain 3:  (?, 6, 6, 64)\n",
      "inception_pool Chain 4:  (?, 6, 6, 128)\n",
      "inception_1x1: Chain 1:  (?, 6, 6, 256)\n",
      "inception4a:  (?, 6, 6, 640)\n",
      "Inside Inception module1:  (?, 6, 6, 640)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 256)\n",
      "inception_5x5 Chain 3:  (?, 3, 3, 128)\n",
      "pool_pad Chain 4:  (?, 3, 3, 640)\n",
      "inception4e:  (?, 3, 3, 1024)\n",
      "Inside Inception module1:  (?, 3, 3, 1024)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5a:  (?, 3, 3, 736)\n",
      "Inside Inception module1:  (?, 3, 3, 736)\n",
      "inception_3x3 Chain 2:  (?, 3, 3, 384)\n",
      "inception_pool Chain 4:  (?, 3, 3, 96)\n",
      "inception_1x1: Chain 1:  (?, 3, 3, 256)\n",
      "inception5b:  (?, 3, 3, 736)\n",
      "X after FC pool:  (?, 1, 1, 736)\n",
      "X after X Flattened:  (?, 736)\n",
      "X after FC Matmul:  (?, 128)\n",
      "(1, 96, 96, 3)\n",
      "(1, 128)\n"
     ]
    }
   ],
   "source": [
    "picPath = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceRecognition/camera_0.jpg'\n",
    "\n",
    "from scipy import misc\n",
    "from nn.Network import getModel\n",
    "\n",
    "# Read an Image\n",
    "def readImage(imagePath):\n",
    "    '''\n",
    "        The input data is is in the shape of [nh, nw, nc], convert it to [nc, nh, nw]\n",
    "    '''\n",
    "    image = misc.imread(picPath)\n",
    "    \n",
    "    img = np.around(image/255.0, decimals=12) #(2,0,1) = [nc, nh, nw]\n",
    "    print (image.shape)\n",
    "    return img\n",
    "\n",
    "img = readImage(picPath)\n",
    "\n",
    "# Reset previously built graph\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# Get the inception Model\n",
    "reset_graph()\n",
    "tensorDict = getModel(img.shape, params=moduleWeightDict)#(img, conv1_w, conv1_b, s=2, pad='SAME',  scope_name='conv1', isTrainable=False)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print (img.shape)\n",
    "    img1 = np.reshape(img, (1,img.shape[0], img.shape[1], img.shape[2]))\n",
    "    print (img1.shape)\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:img1})\n",
    "    print (x[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ROUGH:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FROM THE FACE NET PAPER\n",
    "\n",
    "Choosing triplets (anchor, positive and negative) randomly then the loss function constraint is \n",
    "easily satisfied and we may end up learning a particular type of image and make our mode prone to \n",
    "overfitting the data.\n",
    "\n",
    "So we choose triplets that are hard to train on such that select an anchor compare to all pasotive\n",
    "examples and select the one whose encoding is furthest form the anchor, this is called Hardest Positive. \n",
    "Similarly while selecting a negative image select the image whose encoding is the closest to the \n",
    "achor image, this is called as selecting hardest negative.\n",
    "\n",
    "In practise it is often preferable to we use all anchor- positive pairs in a mini-batch \n",
    "while still selecting the hard negatives.\n",
    "\n",
    "* To Do's\n",
    "   * Extract faces out of the images using Haar Cascades\n",
    "   * Create encoding for each images and test if the encoding are by themself able to distinguish between \n",
    "     difrerent people\n",
    "   * Then check on Fine tuning the network:\n",
    "   * Creating Triplet batches.\n",
    "      * First we create all pairs of ancors and positive.\n",
    "      * Then we create all negative examples for each (anchor and positive), where the triplet violates.\n",
    "      * For a incoming batch, we choise 1st record as anchor, get all its positive pairs, then\n",
    "        we randomly sample a negative example from the negatve points found in prvious step and calculate\n",
    "        the triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "/opt/concourse/volumes/live/65bc8d8a-a7db-4b09-4d80-394825911c99/volume/opencv_1512681450376/work/modules/imgproc/src/color.cpp:11016: error: (-215) scn == 3 || scn == 4 in function cvtColor\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-75e40f6548d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimagePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/original/1.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimgRGB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# gray = cv2.cvtColor(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /opt/concourse/volumes/live/65bc8d8a-a7db-4b09-4d80-394825911c99/volume/opencv_1512681450376/work/modules/imgproc/src/color.cpp:11016: error: (-215) scn == 3 || scn == 4 in function cvtColor\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier('haarcascade_eye.xml')\n",
    "\n",
    "imagePath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/original/1.png\"\n",
    "img = cv2.imread(imagePath)\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "print (img)\n",
    "# gray = cv2.cvtColor(img)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "/Users/jenkins/miniconda/1/x64/conda-bld/conda_1486587097465/work/opencv-3.1.0/modules/highgui/src/window.cpp:281: error: (-215) size.width>0 && size.height>0 in function imshow\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-60ac6371abda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: /Users/jenkins/miniconda/1/x64/conda-bld/conda_1486587097465/work/opencv-3.1.0/modules/highgui/src/window.cpp:281: error: (-215) size.width>0 && size.height>0 in function imshow\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow('image',img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "a = np.array([[1,2],[4,2],[1,1],[6,2],[4,3],[8,4],[1,2]])\n",
    "a_tf = tf.cast(a, dtype=tf.float32)\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "# reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44721359  0.89442718]\n",
      " [ 0.89442718  0.44721359]\n",
      " [ 0.70710671  0.70710671]\n",
      " [ 0.94868326  0.31622776]\n",
      " [ 0.80000001  0.60000002]\n",
      " [ 0.89442718  0.44721359]\n",
      " [ 0.44721359  0.89442718]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    b = tf.nn.l2_normalize(a_tf, dim=1, epsilon=1e-5)\n",
    "    print (b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.89442719,  0.4472136 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([8,4])\n",
    "a / pow(max(sum(a**2), 1e-5), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
