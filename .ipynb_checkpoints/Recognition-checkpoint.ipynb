{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from data_transformer.data_formatter import DataFormatter\n",
    "from data_transformer import preprocess\n",
    "from data_transformer.data_prep import DataIO, genDistinctStratifiedBatches, genRandomStratifiedBatches\n",
    "from nn.load_params import layer_name, convShape, getWeights\n",
    "from nn.utils import getTriplets, tripletLoss\n",
    "from train_test.model import *\n",
    "from config import path_dict\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"logfile.log\", filemode=\"w\",\n",
    "                    format=\"%(asctime)-15s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONE TIME RUN: \n",
    "------------\n",
    "#### INPUT :  Folder path with images of several people, ensure the image folders are named with the person name\n",
    "#### OUTPUT: Dumps a pickle file with three keys, dataX, dataY, labelDict. \n",
    "            * dataX: images converted into nd array\n",
    "            * dataY: for each record of nd array, Labels are numerical (1,2,3,4,5)\n",
    "            * labelDict: Contains the label corresponding to person name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training = True\n",
    "# verification = True\n",
    "\n",
    "# if training:\n",
    "#     objDP = DataFormatter(path_dict['parent_path'], 'training')\n",
    "#     objDP.createResizedData()\n",
    "#     dataX, dataY, labelDict = objDP.imageToArray()\n",
    "#     DataFormatter.dumpPickleFile(dataX, dataY, labelDict,\n",
    "#                                folderPath=os.path.join(path_dict['data_model_path']),\n",
    "#                                picklefileName='training_imgarr.pickle')\n",
    "# if verification:\n",
    "#     objDP = DataFormatter(path_dict['parent_path'], 'verification')\n",
    "#     objDP.createResizedData()\n",
    "#     dataX, dataY, labelDict = objDP.imageToArray()\n",
    "#     DataFormatter.dumpPickleFile(dataX, dataY, labelDict,\n",
    "#                                folderPath=os.path.join(path_dict['data_model_path']),\n",
    "#                                picklefileName='verification_imgarr.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE RANDOM BATCHES:\n",
    "----------------\n",
    "#### INPUT: Image nd array as input  [num_images, imgX, imgY, num_channels]\n",
    "#### OUTPUT: Outputs a pickle file with shape [num_batches, num_image_per_batch, imgX, imgY, num_channels]\n",
    "\n",
    "       *  We would wanna do stocastic descent for minibatches and update the parameters perbatch. This module attempts to create stratified batches (each batch would have equal distribution of labels). \n",
    "       \n",
    "       * when genDistinctStratifiedBatches. The images in the batched would be distinct (would not repeat)\n",
    "       * when genRandomStratifiedBatches. No seed is set for shuffling. So Images in different batches may repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/training_imgarr.pickle\n",
      "/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/verification_imgarr.pickle\n",
      "(180, 96, 96, 3) (180, 1)\n",
      "(3, 96, 96, 3) (3, 1)\n"
     ]
    }
   ],
   "source": [
    "debugg = True\n",
    "imgPerLabels = 6 # num image per label per batch\n",
    "numBatches = 10\n",
    "if debugg:\n",
    "    trainX, trainY, trainLabelDict = DataIO.getPickleFile(path_dict['data_model_path'],\n",
    "                                                                 'training_imgarr.pickle')\n",
    "    verX, verY, verLabelDict = DataIO.getPickleFile(path_dict['data_model_path'],\n",
    "                                                           'verification_imgarr.pickle')\n",
    "    print(trainX.shape, trainY.shape)\n",
    "    print(verX.shape, verY.shape)\n",
    "    genDistinctStratifiedBatches(trainX, trainY, numImgsPerLabels=60, numBatches=numBatches,\n",
    "                          fileName='distinct_stratified_batches.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET INCEPTION WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "moduleWeightDict = getWeights(path_dict['inception_nn4small_weights_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO's:\n",
    "\n",
    "1. Remove the random weight initialiazer for the last layer, and initialize it \n",
    "   with the inception net weights.  DONE\n",
    "\n",
    "2. implement a module to save weights as checkpoints to the disk.  DONE\n",
    "\n",
    "3. create a function to toggle between Random weight initializer, Inception net weight initializer \n",
    "   and using the saved checkpoint for the last Inception layer. DONE\n",
    "   \n",
    "4.0 : REMEBER TO STORE THE exponential weighted average of mean and variable in the batch normalization \n",
    "      fine tune function. SET THESE AS A VARIABLE (LOOK AT CIFAR CODE FOR HELP)\n",
    " \n",
    "4. Add more images.\n",
    "\n",
    "5. Create a complete workflow train the network and see\n",
    "\n",
    "6. Store 1 image encodings for the 3-4 labels you have.\n",
    "\n",
    "7. For a new image, pass the image throught network, get the encoding and see which is the most closest face using the encoding from the step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "class SVM():\n",
    "    '''\n",
    "    # The embeddings in a nutshell are features. The face image goes through a complex network and results in\n",
    "    # embeddings that captures complex features of a face. SVM's are good at classifying small datasets.\n",
    "    # SVM are also robust to over fitting. The idea here is that we would wanna learn a SVM classifier using the\n",
    "    # embeddings as the feature space and see for the given embedding, how many times we are able to predict the\n",
    "    # correct class\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, embeddings, labels, model_name=None):\n",
    "        '''\n",
    "        :param embeddings:   Embeddings of the image\n",
    "        :param labels:       labels\n",
    "        :return:\n",
    "        '''\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(embeddings, labels)\n",
    "        joblib.dump(model, os.path.join(path_dict['classification_model_path'], str(model_name)+\"_svm.sav\"))\n",
    "        \n",
    "    def classify(self, embeddings, model_name=None):\n",
    "        '''\n",
    "        :param embeddings: Image embeddings to classify\n",
    "        :param model_name:\n",
    "        :return:\n",
    "        '''\n",
    "        model = joblib.load(os.path.join(path_dict['classification_model_path'], str(model_name)+\"_svm.sav\"))\n",
    "        predLabels = model.predict_proba(embeddings)\n",
    "        top_label_idx = np.argmax(predLabels, axis=1)\n",
    "        labelProb = predLabels[np.arange(len(top_label_idx)), top_label_idx]\n",
    "        return top_label_idx, labelProb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input data (X) is:  (10, 18, 96, 96, 3)\n",
      "The shape of input data (Y) is:  (10, 18)\n",
      "Unique labels in dataY is:  [ 0.  1.  2.]\n",
      "Label dict:  None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/saver_checkpoints/model.ckpt\n",
      "Fold: 1, Epoch= 1, Loss= 1.565698\n",
      "0.83950617284\n",
      "0.666666666667\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "0-th value returned by pyfunc_0 is double, but expects int64\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_INT64], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionFC_FT/L2_norm, PyFunc/input_1, PyFunc/input_2, PyFunc/input_3)]]\n\nCaused by op 'PyFunc', defined at:\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-baed4ea602a3>\", line 138, in <module>\n    objExec.run()\n  File \"<ipython-input-7-baed4ea602a3>\", line 83, in run\n    trn_embed_graph = trainEmbeddings(moduleWeightDict,init_wght_type='random')\n  File \"/Users/sam/All-Program/App/DeepFaceRecognition/train_test/model.py\", line 63, in trainEmbeddings\n    embeddingDict = loss(embeddingDict, img_per_label, num_labels, alpha)\n  File \"/Users/sam/All-Program/App/DeepFaceRecognition/nn/network.py\", line 313, in loss\n    tf.int64)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 212, in py_func\n    input=inp, token=token, Tout=Tout, name=name)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 50, in _py_func\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): 0-th value returned by pyfunc_0 is double, but expects int64\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_INT64], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionFC_FT/L2_norm, PyFunc/input_1, PyFunc/input_2, PyFunc/input_3)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 0-th value returned by pyfunc_0 is double, but expects int64\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_INT64], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionFC_FT/L2_norm, PyFunc/input_1, PyFunc/input_2, PyFunc/input_3)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-baed4ea602a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mobjExec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmoduleWeightDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddingType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'finetune'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0mobjExec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-baed4ea602a3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         opt, batch_loss = sess.run([trn_embed_graph['optimizer'], \n\u001b[1;32m    112\u001b[0m                                                     trn_embed_graph['loss']], \n\u001b[0;32m--> 113\u001b[0;31m                                                     feed_dict={trn_embed_graph['inpTensor']:batchX})\n\u001b[0m\u001b[1;32m    114\u001b[0m                     print(\"Fold: \" + str(self.nFold) + \n\u001b[1;32m    115\u001b[0m                           \u001b[0;34m\", Epoch= \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 0-th value returned by pyfunc_0 is double, but expects int64\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_INT64], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionFC_FT/L2_norm, PyFunc/input_1, PyFunc/input_2, PyFunc/input_3)]]\n\nCaused by op 'PyFunc', defined at:\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2808, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-baed4ea602a3>\", line 138, in <module>\n    objExec.run()\n  File \"<ipython-input-7-baed4ea602a3>\", line 83, in run\n    trn_embed_graph = trainEmbeddings(moduleWeightDict,init_wght_type='random')\n  File \"/Users/sam/All-Program/App/DeepFaceRecognition/train_test/model.py\", line 63, in trainEmbeddings\n    embeddingDict = loss(embeddingDict, img_per_label, num_labels, alpha)\n  File \"/Users/sam/All-Program/App/DeepFaceRecognition/nn/network.py\", line 313, in loss\n    tf.int64)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/script_ops.py\", line 212, in py_func\n    input=inp, token=token, Tout=Tout, name=name)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 50, in _py_func\n    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): 0-th value returned by pyfunc_0 is double, but expects int64\n\t [[Node: PyFunc = PyFunc[Tin=[DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT], Tout=[DT_INT64], token=\"pyfunc_0\", _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](InceptionFC_FT/L2_norm, PyFunc/input_1, PyFunc/input_2, PyFunc/input_3)]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "\n",
    "path_to_batch_data = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/batch_img_arr'\n",
    "batch_file_name = 'random_batches.pickle'\n",
    "checkpoint_path = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/saver_checkpoints'\n",
    "\n",
    "\n",
    "'''\n",
    "dataX = [num_batches, image_per_batch, image_x, image_y, image_channels]\n",
    "dataY = [num_batches, labels]\n",
    "\n",
    "'''\n",
    "\n",
    "class Execute():\n",
    "    def __init__(self, params, embeddingType='finetune'):\n",
    "        self.params = params\n",
    "        self.embeddingType = embeddingType\n",
    "        \n",
    "    def runPreprocessor(self, dataIN, sess):\n",
    "        preprocessedData = np.ndarray(shape=(dataIN.shape), dtype='float32')\n",
    "        for numImage in np.arange(dataIN.shape[0]):\n",
    "            feed_dict = {\n",
    "                self.preprocessGraphDict['imageIN']:dataIN[numImage,:]\n",
    "            }\n",
    "            preprocessedData[numImage,:] = sess.run(self.preprocessGraphDict['imageOUT'],\n",
    "                                                      feed_dict=feed_dict)\n",
    "        return preprocessedData\n",
    "        \n",
    "    def setNewWeights(self, sess):\n",
    "        trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        if self.embeddingType=='finetune':\n",
    "            for var in trainableVars:\n",
    "                scope, name = var.name.split(':')[0].split('/')\n",
    "                if len(self.params[scope][name]) != 0:\n",
    "                    var_ = sess.run(var)\n",
    "                    logging.info('Updating param with scope %s and name %s and shape %s with shape %s',\n",
    "                                 str(scope), str(name), str(self.params[scope][name].shape), str(var_.shape))\n",
    "                    self.params[scope][name] = var_\n",
    "                else:\n",
    "                    raise ValueError('It seems that the scope %s or variable %s didnt exist in the dictionary ' % (str(scope), str(name)))\n",
    "    \n",
    "    def train(self, trnX_, trnY_, sess):\n",
    "        '''\n",
    "            1. Make the use of getEmbedding to get the graph with last layer parameter updated with the \n",
    "            fine tuned weights.\n",
    "            2. Get the new embedding for batch/epoch using the computation graph\n",
    "            3. Use the embeddings as feature for a classifier (svm/softmax)\n",
    "            4. Classify faces using the new embeddings.\n",
    "        '''\n",
    "        trainEmbedGraph = getEmbeddings([96,96,3], self.params)\n",
    "        embeddings = sess.run(trainEmbedGraph['output'], \n",
    "                              feed_dict={trainEmbedGraph['inpTensor']:trnX_})\n",
    "        logging.info('Training Embeddings shape %s', embeddings.shape)\n",
    "        obj_svm = SVM()\n",
    "        obj_svm.train(embeddings, labels=trnY_, \n",
    "                      model_name='nFold_%s_batch_%s'%(str(self.nFold),str(self.epoch)))\n",
    "        train_labels, train_label_prob = obj_svm.classify(embeddings, \n",
    "                                             model_name='nFold_%s_batch_%s'%(str(self.nFold),str(self.epoch)))\n",
    "        return train_labels, train_label_prob\n",
    "    \n",
    "    def cvalid(self, cvX_, sess):\n",
    "        embedGraph = getEmbeddings([96,96,3], self.params)\n",
    "        embeddings = sess.run(embedGraph['output'], \n",
    "                              feed_dict={embedGraph['inpTensor']:cvX_})\n",
    "        logging.info('Cross validation Embeddings shape %s', embeddings.shape)\n",
    "        obj_svm = SVM()\n",
    "        cv_labels, cv_label_prob = obj_svm.classify(embeddings, \n",
    "                                             model_name='nFold_%s_batch_%s'%(str(self.nFold),str(self.epoch)))\n",
    "        return cv_labels, cv_label_prob\n",
    "    \n",
    "    def accuracy(self, y, y_hat):\n",
    "        return np.mean(np.equal(y_hat, y))\n",
    "    \n",
    "#     def test(self, tstGraph, testBatch, sess):\n",
    "#         # METHOD 2: TO get weights is form of Tensors\n",
    "#         a = saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "#         trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#         testDict = getFineTunedEmbeddings([96,96,3], moduleWeightDict, trainableVars, sess)\n",
    "#         embeddings = sess.run([tstGraph['output']], feed_dict={'inpTensor':testBatch})\n",
    "#         return embeddings\n",
    "\n",
    "    def run(self):\n",
    "        # GET THE BATCH DATA FROM THE DISK\n",
    "        dataX, dataY, labelDict = DataFormatter.getPickleFile(\n",
    "            folderPath=path_to_batch_data, picklefileName=batch_file_name, getStats=True\n",
    "        )\n",
    "        trnBatch_idx = [list(np.setdiff1d(np.arange(len(dataX)), np.array(i))) for i in  np.arange(len(dataX))]\n",
    "        cvBatch_idx = [i for i in  np.arange(len(dataX))]\n",
    "        logging.info('dataX.shape = %s, dataY.shape = %s',str(dataX.shape), str(dataY.shape))\n",
    "\n",
    "        # Reset graph to do a fresh start\n",
    "        reset_graph()\n",
    "        trn_embed_graph = trainEmbeddings(moduleWeightDict,init_wght_type='random')\n",
    "        self.preprocessGraphDict = Preprocessing().preprocessImageGraph(\n",
    "                                                            imageSize=self.myNet[\"imageSize\"],\n",
    "                                                            numChannels=self.myNet[\"numChannels\"])\n",
    "        # add ops to save and restore model\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            checkpoints = [ck for ck in os.listdir(checkpoint_path) if ck!='.DS_Store']\n",
    "            if len(checkpoints) > 0 :\n",
    "                saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "\n",
    "            # LOOP FOR N-FOLD CROSS VALIDATION\n",
    "            for nFold, (trn_batch_idx, cv_batch_idx) in enumerate(zip(trnBatch_idx, cvBatch_idx)):\n",
    "                self.nFold = nFold + 1\n",
    "                logging.info('RUNNING : %s FOLD ...........................', str(self.nFold))\n",
    "                trnX = dataX[trn_batch_idx,:]\n",
    "                trnY = dataY[trn_batch_idx,:]\n",
    "                cvX = dataX[cv_batch_idx,:]\n",
    "                cvY = dataY[cv_batch_idx,:]\n",
    "                logging.info('trnX.shape = %s, trnY.shape = %s, cvX.shape = %s, cvY.shape = %s', \n",
    "                      str(trnX.shape), str(trnY.shape), str(cvX.shape), str(cvY.shape))\n",
    "\n",
    "                for epoch in np.arange(2):\n",
    "                    self.epoch = epoch + 1\n",
    "                    logging.info('RUNNING : %s EPOCH ........................', str(self.epoch))\n",
    "                    # Below loop will minimize the triplet loss and update the parameters\n",
    "                    for batchNum, batchX in enumerate(trnX[0:len(trnX),:]):\n",
    "                        logging.info('RUNNING BATCH shape = %s', str(batchX.shape))\n",
    "                        \n",
    "                        # Step1 : Preprocess the Data\n",
    "                        preprocessedData = self.runPreprocessor(dataIN=batchX, sess=sess)\n",
    "                    \n",
    "                        opt, batch_loss = sess.run([trn_embed_graph['optimizer'], \n",
    "                                                    trn_embed_graph['loss']], \n",
    "                                                    feed_dict={trn_embed_graph['inpTensor']:preprocessedData})\n",
    "                    print(\"Fold: \" + str(self.nFold) + \n",
    "                          \", Epoch= \" + str(self.epoch) + \n",
    "                          \", Loss= \" + \"{:.6f}\".format(batch_loss))\n",
    "                    save_path = saver.save(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "\n",
    "                    # Now that we have updated our parameters (weights and biases), we would\n",
    "                    # fetch the embeddings using the updated parameter and train-test model\n",
    "                    # to get an accuracy. Accuracy per epoch is now a good way to go\n",
    "                    self.setNewWeights(sess) # replace the last layer's inception weights with leared finetuned weights\n",
    "                    \n",
    "                    # TRAIN, GET TRAINING PREDICTION AND ACCURACY\n",
    "                    trnX_ = trnX.reshape(-1, trnX.shape[2], trnX.shape[3], trnX.shape[4]) # accumulate all batches\n",
    "                    trnY_ = trnY.flatten()\n",
    "                    train_labels, _ = self.train(trnX_, trnY_, sess)\n",
    "                    tr_acc = self.accuracy(y=trnY_, y_hat=train_labels)\n",
    "                    print (tr_acc)\n",
    "                    \n",
    "                    # GET CROSS VALIDATION PREDICTION AND ACCURACY\n",
    "                    cv_labels, _ = self.cvalid(cvX, sess)\n",
    "                    cv_acc = self.accuracy(y=cvY, y_hat=cv_labels)\n",
    "                    print (cv_acc)\n",
    "                break\n",
    "\n",
    "objExec = Execute(params=moduleWeightDict, embeddingType='finetune')\n",
    "objExec.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGH\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "tfdata = tf.cast(np.random.rand(1,1,3,5) + 10, dtype=tf.float32)\n",
    "print (tfdata.get_shape().as_list())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batchMean, batchVar = tf.nn.moments(tfdata, axes=[0,1,2], name=\"moments\")\n",
    "    print (tfdata.eval())\n",
    "    print (batchMean.eval())\n",
    "    print (batchMean.get_shape().as_list())\n",
    "    print (batchVar.eval())\n",
    "    print (batchVar.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
