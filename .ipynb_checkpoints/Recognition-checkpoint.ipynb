{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from DataPrep.data_io import DataFormatter\n",
    "from nn.load_params import layer_name, convShape, getWeights\n",
    "from nn.utils import getTriplets, tripletLoss\n",
    "# from nn.model import initNetwork\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"logfile.log\", filemode=\"w\",\n",
    "                    format=\"%(asctime)-15s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET INCEPTION WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE NETWORK WITHWEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorDict = initNetwork(moduleWeightDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE BATCH DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def getTriplets(batch_embedding):\n",
    "    img_per_label=6\n",
    "    num_labels=3 \n",
    "    alpha=0.01\n",
    "    batch_tripet_idx = []\n",
    "    idx_arr = np.arange(len(batch_embedding))\n",
    "    for i in np.arange(num_labels):\n",
    "        pos_idxs = np.arange( i *img_per_label , i * img_per_label +img_per_label)\n",
    "        neg_idxs = np.setdiff1d(idx_arr, pos_idxs)\n",
    "        # print(pos_idxs)\n",
    "        # print('')\n",
    "        # print(neg_idxs)\n",
    "        compare_point = -1  # used to avoid redundancy in calculating SSE between anchor and all negative\n",
    "        # Get all combination of Anchor and positive\n",
    "        # print ('######################################')\n",
    "        for anc_idx, pos_idx in combinations(pos_idxs, 2):\n",
    "            if anc_idx != compare_point:\n",
    "                compare_point += 1\n",
    "                # Get the sum of squared distance between anchor and positive\n",
    "                anc_VS_neg_ndarr = np.sum(np.square(\n",
    "                        batch_embedding[anc_idx] - batch_embedding[neg_idxs]), 1\n",
    "                )\n",
    "                # print (anc_VS_neg_ndarr)\n",
    "            \n",
    "            # Get the sum of squared distance between anchor and positive\n",
    "            anc_VS_pos = np.sum(\n",
    "                    np.square(batch_embedding[anc_idx] - batch_embedding[pos_idx]))\n",
    "            # print ('anc_VS_pos anc_VS_pos', anc_VS_pos)\n",
    "            \n",
    "            hard_neg_idx = np.where(anc_VS_neg_ndarr - anc_VS_pos < alpha)[0]\n",
    "            # print (hard_neg_idx)\n",
    "            \n",
    "            # Randomly sample 1 record from the hard negative idx, and create a triplet\n",
    "            if len(hard_neg_idx) >0:\n",
    "                # print('hard_neg_idx hard_neg_idx ', hard_neg_idx)\n",
    "                np.random.shuffle(hard_neg_idx)\n",
    "                rnd_idx = hard_neg_idx[0]\n",
    "                # print ('neg_idx neg_idx ', neg_idxs[rnd_idx])\n",
    "                \n",
    "                # Get triplet indexes in order to work on offline mode\n",
    "                batch_tripet_idx.append([anc_idx, pos_idx, neg_idxs[rnd_idx]])\n",
    "                # print (anc_idx,pos_idx)\n",
    "                # print ('')\n",
    "                # print (break_point - cnt)\n",
    "    # print('TRIPLET INDEX ', batch_tripet_idx)\n",
    "    \n",
    "    return [batch_tripet_idx]\n",
    "\n",
    "\n",
    "def loss(encodingDict):\n",
    "    tripletIDX = tf.py_func(getTriplets, [encodingDict['output']], tf.int64)\n",
    "    loss = tripletLoss(\n",
    "        tf.gather(tf.cast(encodingDict['output'], dtype=tf.float32), tripletIDX[:,0]),\n",
    "        tf.gather(tf.cast(encodingDict['output'], dtype=tf.float32), tripletIDX[:,1]),\n",
    "        tf.gather(tf.cast(encodingDict['output'], dtype=tf.float32), tripletIDX[:,2]),\n",
    "        alpha=0.2)\n",
    "    encodingDict['tripletIDX'] = tripletIDX\n",
    "    encodingDict['loss'] = loss\n",
    "    return encodingDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def getBatchTriplets():\n",
    "#     getTriplets_TF(batch_embedding, img_per_label, num_labels, alpha=0.01)\n",
    "    \n",
    "from nn.model import getModel_FT, getModel\n",
    "def initNetwork(weightDict, isTrainable=False):\n",
    "    logging.info('INITIALIZING THE NETWORK !! ...............................')\n",
    "    if not isTrainable:\n",
    "        encodingDict = getModel([96, 96, 3], params=weightDict)\n",
    "    else:\n",
    "        encodingDict = getModel_FT([96, 96, 3], params=weightDict)   \n",
    "        encodingDict = loss(encodingDict)\n",
    "    return encodingDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input data (X) is:  (10, 18, 96, 96, 3)\n",
      "The shape of input data (Y) is:  (10, 18)\n",
      "Unique labels in dataY is:  [ 0.  1.  2.]\n",
      "Label dict:  None\n",
      "{'output': <tf.Tensor 'InceptionFC_FT/L2_norm:0' shape=(?, 128) dtype=float32>, 'loss': <tf.Tensor 'TripletLoss/Sum_2:0' shape=<unknown> dtype=float32>, 'tripletIDX': <tf.Tensor 'PyFunc:0' shape=<unknown> dtype=int64>, 'inpTensor': <tf.Tensor 'Placeholder:0' shape=(?, 96, 96, 3) dtype=float32>}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'anchor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-180f8de091a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mbatchY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatchNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         embeddings_idx, anchor = sess.run([encodingDict['tripletIDX'], \n\u001b[0;32m---> 22\u001b[0;31m                                    encodingDict['anchor']], \n\u001b[0m\u001b[1;32m     23\u001b[0m                                   feed_dict={encodingDict['inpTensor']:batchX})\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membeddings_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'anchor'"
     ]
    }
   ],
   "source": [
    "path_to_batch_data = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/batch_img_arr'\n",
    "batch_file_name = 'random_batches.pickle'\n",
    "\n",
    "dataX, dataY, labelDict = DataFormatter.getPickleFile(folderPath=path_to_batch_data, \n",
    "                                                      picklefileName=batch_file_name, getStats=True)\n",
    "\n",
    "\n",
    "def optimize(loss):\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=0.1)\n",
    "    grads = opt.compute_gradients(loss, tf.global_variables())\n",
    "    apply_gradient_op = opt.apply_gradients(grads)\n",
    "    return apply_gradient_op\n",
    "\n",
    "reset_graph()\n",
    "encodingDict = initNetwork(moduleWeightDict, isTrainable=True)\n",
    "print (encodingDict)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for batchNum, batchX in enumerate(dataX):\n",
    "        batchY = dataY[batchNum, :]\n",
    "        embeddings_idx, anchor = sess.run([encodingDict['tripletIDX'], \n",
    "                                   encodingDict['loss']], \n",
    "                                  feed_dict={encodingDict['inpTensor']:batchX})\n",
    "        print (embeddings_idx)\n",
    "        print('@@@@')\n",
    "        print (anchor)\n",
    "        print ('#########################')\n",
    "#         embeddings = embeddings[0]\n",
    "        \n",
    "#         a\n",
    "#         tripletIndexArr = np.array(tripletIndexArr).reshape(-1,3)\n",
    "#         a_idxs = tripletIndexArr[:,0].flatten()\n",
    "#         p_idxs = tripletIndexArr[:,1].flatten()\n",
    "#         n_idxs = tripletIndexArr[:,2].flatten()\n",
    "#         loss = tripletLoss(tf.gather(tf.cast(embeddings, dtype=tf.float32), a_idxs),\n",
    "#                            tf.gather(tf.cast(embeddings, dtype=tf.float32), p_idxs),\n",
    "#                            tf.gather(tf.cast(embeddings, dtype=tf.float32), n_idxs), alpha=0.2)\n",
    "#         print (loss.eval())\n",
    "        \n",
    "        \n",
    "#         pritn ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "tfdata = tf.cast(np.random.rand(1,1,3,5) + 10, dtype=tf.float32)\n",
    "print (tfdata.get_shape().as_list())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batchMean, batchVar = tf.nn.moments(tfdata, axes=[0,1,2], name=\"moments\")\n",
    "    print (tfdata.eval())\n",
    "    print (batchMean.eval())\n",
    "    print (batchMean.get_shape().as_list())\n",
    "    print (batchVar.eval())\n",
    "    print (batchVar.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "(3, 3)\n",
      "[[1 2 3]\n",
      " [3 2 4]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "\n",
    "a = tf.constant([[1,2,3],[3,2,4],[1,1,4]])\n",
    "print(t.shape)\n",
    "print (a.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (tf.gather(a, [0, 1]).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3558921769"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(0.3426 - 0.927, 2) + pow(0.7484-0.62853, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
