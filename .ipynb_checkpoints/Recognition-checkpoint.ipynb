{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from DataPrep.data_io import DataFormatter\n",
    "from nn.load_params import layer_name, convShape, getWeights\n",
    "from nn.utils import getTriplets, tripletLoss\n",
    "from train_test.model import *\n",
    "# from train_test.model import initNetwork\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"logfile.log\", filemode=\"w\",\n",
    "                    format=\"%(asctime)-15s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET INCEPTION WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w', 'm', 'v', 'b'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moduleWeightDict['inception_3c_5x5_bn2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for i in moduleWeightDict['dense'].keys():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO's:\n",
    "\n",
    "1. Remove the random weight initialiazer for the last layer, and initialize it \n",
    "   with the inception net weights.  DONE\n",
    "\n",
    "2. implement a module to save weights as checkpoints to the disk.  DONE\n",
    "\n",
    "3. create a function to toggle between Random weight initializer, Inception net weight initializer \n",
    "   and using the saved checkpoint for the last Inception layer. DONE\n",
    "   \n",
    "4.0 : REMEBER TO STORE THE exponential weighted average of mean and variable in the batch normalization \n",
    "      fine tune function. SET THESE AS A VARIABLE (LOOK AT CIFAR CODE FOR HELP)\n",
    " \n",
    "4. Add more images.\n",
    "\n",
    "5. Create a complete workflow train the network and see\n",
    "\n",
    "6. Store 1 image encodings for the 3-4 labels you have.\n",
    "\n",
    "7. For a new image, pass the image throught network, get the encoding and see which is the most closest face using the encoding from the step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "where_to_dump_model = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/classification_model'\n",
    "\n",
    "class SVM():\n",
    "    '''\n",
    "    # The embeddings in a nutshell are features. The face image goes through a complex network and results in\n",
    "    # embeddings that captures complex features of a face. SVM's are good at classifying small datasets.\n",
    "    # SVM are also robust to over fitting. The idea here is that we would wanna learn a SVM classifier using the\n",
    "    # embeddings as the feature space and see for the given embedding, how many times we are able to predict the\n",
    "    # correct class\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, embeddings, labels, iter_num=None):\n",
    "        '''\n",
    "        :param embeddings:   Embeddings of the image\n",
    "        :param labels:       labels\n",
    "        :return:\n",
    "        '''\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(embeddings, labels)\n",
    "        joblib.dump(model, os.path.join(where_to_dump_model, iter_num+\"_svm.sav\"))\n",
    "        \n",
    "    def classify(self, embeddings, iter_num=None):\n",
    "        '''\n",
    "        :param embeddings: Image embeddings to classify\n",
    "        :param iter_num:\n",
    "        :return:\n",
    "        '''\n",
    "        model = joblib.load(os.path.join(where_to_dump_model, iter_num+\"_svm.sav\"))\n",
    "        predLabels = model.predict_proba(embeddings)\n",
    "        best_label_idx = np.argmax(predLabels, axis=1)\n",
    "        print (predLabels)\n",
    "        print (best_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input data (X) is:  (10, 18, 96, 96, 3)\n",
      "The shape of input data (Y) is:  (10, 18)\n",
      "Unique labels in dataY is:  [ 0.  1.  2.]\n",
      "Label dict:  None\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9], [0, 2, 3, 4, 5, 6, 7, 8, 9], [0, 1, 3, 4, 5, 6, 7, 8, 9], [0, 1, 2, 4, 5, 6, 7, 8, 9], [0, 1, 2, 3, 5, 6, 7, 8, 9], [0, 1, 2, 3, 4, 6, 7, 8, 9], [0, 1, 2, 3, 4, 5, 7, 8, 9], [0, 1, 2, 3, 4, 5, 6, 8, 9], [0, 1, 2, 3, 4, 5, 6, 7, 9], [0, 1, 2, 3, 4, 5, 6, 7, 8]]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "(10, 18, 96, 96, 3) (10, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.]]\n",
      "(9, 18, 96, 96, 3) (9, 18) (18, 96, 96, 3) (18,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "iteration over a 0-d array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-96a6721ed7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-96a6721ed7df>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrnX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrnY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;31m# Below loop will minimize the triplet loss and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mbatchNum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrnX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrnX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d array"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "path_to_batch_data = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/batch_img_arr'\n",
    "batch_file_name = 'random_batches.pickle'\n",
    "checkpoint_path = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/saver_checkpoints'\n",
    "\n",
    "\n",
    "'''\n",
    "dataX = [num_batches, image_per_batch, image_x, image_y, image_channels]\n",
    "dataY = [num_batches, labels]\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "    ########  Code for Testing the model\n",
    "    # METHOD 1: TO get weights is form of Tensors\n",
    "#     saver = tf.train.import_meta_graph(os.path.join(checkpoint_path, \"model.ckpt.meta\"))\n",
    "#     saver.restore(sess,tf.train.latest_checkpoint(checkpoint_path))\n",
    "#     graph = tf.get_default_graph()\n",
    "#     w = graph.get_tensor_by_name(\"inception_5a_3x3_conv1/w:0\")\n",
    "#     print (w.shape)\n",
    "\n",
    "class Execute():\n",
    "    def __init__(params, embeddingType='finetune'):\n",
    "        self.params = params\n",
    "        self.embeddingType = embeddingType\n",
    "        \n",
    "    def setNewWeights():\n",
    "        trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        if embeddingType=='finetune':\n",
    "            for var in trainableVars:\n",
    "                scope, name = var.name.split(':')[0].split('/')\n",
    "                if len(params[scope][name]) != 0:\n",
    "                    var_ = sess.run(var)\n",
    "                    logging.info('Updating param with scope %s and name %s and shape %s with shape %s',\n",
    "                                 str(scope), str(name), str(params[scope][name].shape), str(var_.shape))\n",
    "                    params[scope][name] = var_\n",
    "                else:\n",
    "                    print('It seems that the scope %s or variable %s didnt exist in the dictionary ' % (str(scope), str(name)))\n",
    "    \n",
    "#     def train(self, trnGraph, trnX, trnY, sess):\n",
    "#         a = saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "#         trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#         testDict = getFineTunedEmbeddings([96,96,3], moduleWeightDict, trainableVars, sess)\n",
    "#         embeddings = sess.run([trnGraph['output']], feed_dict={'inpTensor':trnX})\n",
    "#         return embeddings\n",
    "    \n",
    "#     def cvalid(self, cvGraph, cvX, cvY, sess):\n",
    "        \n",
    "\n",
    "#     def test(self, tstGraph, testBatch, sess):\n",
    "#         # METHOD 2: TO get weights is form of Tensors\n",
    "#         a = saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "#         trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "#         testDict = getFineTunedEmbeddings([96,96,3], moduleWeightDict, trainableVars, sess)\n",
    "#         embeddings = sess.run([tstGraph['output']], feed_dict={'inpTensor':testBatch})\n",
    "#         return embeddings\n",
    "\n",
    "    def run(self):\n",
    "        # GET THE BATCH DATA FROM THE DISK\n",
    "        dataX, dataY, labelDict = DataFormatter.getPickleFile(\n",
    "            folderPath=path_to_batch_data, picklefileName=batch_file_name, getStats=True\n",
    "        )\n",
    "        trnBatch_idx = [list(np.setdiff1d(np.arange(len(dataX)), np.array(i))) for i in  np.arange(len(dataX))]\n",
    "        cvBatch_idx = [i for i in  np.arange(len(dataX))]\n",
    "        print (trnBatch_idx)\n",
    "        print (cvBatch_idx)\n",
    "        print (dataX.shape, dataY.shape)\n",
    "\n",
    "        # Reset graph to do a fresh start\n",
    "        reset_graph()\n",
    "        trn_embed_graph = trainEmbeddings(moduleWeightDict,init_wght_type='random')\n",
    "\n",
    "        # add ops to save and restore model\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            checkpoints = [ck for ck in os.listdir(checkpoint_path) if ck!='.DS_Store']\n",
    "            if len(checkpoints) > 0 :\n",
    "                saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "\n",
    "            # LOOP FOR N-FOLD CROSS VALIDATION\n",
    "            for nfold, (trn_batch_idx, cv_batch_idx) in enumerate(zip(trnBatch_idx, cvBatch_idx)):\n",
    "                trnX = dataX[trn_batch_idx,:]\n",
    "                trnY = dataY[trn_batch_idx,:]\n",
    "                print (trnY)\n",
    "                cvX = dataX[cv_batch_idx,:]\n",
    "                cvY = dataY[cv_batch_idx,:]\n",
    "                print (trnX.shape, trnY.shape, cvX.shape, cvY.shape)\n",
    "\n",
    "                for epoch in np.arange(2):\n",
    "                    # Below loop will minimize the triplet loss and update the parameters\n",
    "                    for batchNum, batchX in enumerate(trnX[0:len(trnX),:]):\n",
    "                        batchY = batchX[batchNum, :]\n",
    "                        opt, batch_loss = sess.run([trn_embed_graph['optimizer'], \n",
    "                                                    trn_embed_graph['loss']], \n",
    "                                                    feed_dict={trn_embed_graph['inpTensor']:batchX})\n",
    "                    print(\"Fold: \" + str(nfold + 1) + \n",
    "                          \", Epoch= \" + str(epoch) + \n",
    "                          \", Loss= \" + \"{:.6f}\".format(loss))\n",
    "                    save_path = saver.save(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "\n",
    "                    # Now that we have updated our parameters (weights and biases), we would\n",
    "                    # fetch teh embeddings usig the updated parameter and train-test model\n",
    "                    # to get an accuracy. Accuracy per epoch is now a good way to go\n",
    "                    trnX = trnX.reshape(-1, a.shape[2], a.shape[3], a.shape[4])\n",
    "                break\n",
    "\n",
    "#     run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 2, 1, 2)\n",
      "[[[[[-0.41675785 -0.05626683]]\n",
      "\n",
      "   [[-2.1361961   1.64027081]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[-1.79343559 -0.84174737]]\n",
      "\n",
      "   [[ 0.50288142 -1.24528809]]]]]\n",
      "\n",
      "(2, 4)\n",
      "[[-0.41675785 -0.05626683 -2.1361961   1.64027081]\n",
      " [-1.79343559 -0.84174737  0.50288142 -1.24528809]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "a = np.random.randn(2,1,2,1,2)\n",
    "a_nw = a.reshape(-1, a.shape[2]*a.shape[3]*a.shape[4])\n",
    "print (a.shape)\n",
    "print(a)\n",
    "print ('')\n",
    "\n",
    "print (a_nw.shape)\n",
    "print (a_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 5, 6)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [0, 2, 3, 4], [0, 1, 3, 4], [0, 1, 2, 4], [0, 1, 2, 3]]\n",
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "trnBatch = [list(np.setdiff1d(np.arange(5), np.array(i))) for i in  np.arange(5)]\n",
    "cvBatch = [i for i in  np.arange(5)]\n",
    "print (trnBatch)\n",
    "print (cvBatch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'inception_5a_3x3_conv1_wb/convWeight:0' shape=(1, 1, 1024, 96) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv1_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn1_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn1_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv2_wb/convWeight:0' shape=(3, 3, 96, 384) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv2_wb/convBias:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn2_bn/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn2_bn/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_conv_wb/convWeight:0' shape=(1, 1, 1024, 96) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_conv_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_bn_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_bn_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_conv_wb/convWeight:0' shape=(1, 1, 1024, 256) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_conv_wb/convBias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_bn_bn/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_bn_bn/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv1_wb/convWeight:0' shape=(1, 1, 736, 96) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv1_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn1_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn1_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv2_wb/convWeight:0' shape=(3, 3, 96, 384) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv2_wb/convBias:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn2_bn/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn2_bn/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_conv_wb/convWeight:0' shape=(1, 1, 736, 96) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_conv_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_bn_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_bn_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_conv_wb/convWeight:0' shape=(1, 1, 736, 256) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_conv_wb/convBias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_bn_bn/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_bn_bn/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'InceptionFC_FT_wb/convWeight:0' shape=(736, 128) dtype=float32_ref>, <tf.Variable 'InceptionFC_FT_wb/convBias:0' shape=(128,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print ([v for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "tfdata = tf.cast(np.random.rand(1,1,3,5) + 10, dtype=tf.float32)\n",
    "print (tfdata.get_shape().as_list())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batchMean, batchVar = tf.nn.moments(tfdata, axes=[0,1,2], name=\"moments\")\n",
    "    print (tfdata.eval())\n",
    "    print (batchMean.eval())\n",
    "    print (batchMean.get_shape().as_list())\n",
    "    print (batchVar.eval())\n",
    "    print (batchVar.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "(3, 3)\n",
      "[[1 2 3]\n",
      " [3 2 4]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "\n",
    "a = tf.constant([[1,2,3],[3,2,4],[1,1,4]])\n",
    "print(t.shape)\n",
    "print (a.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (tf.gather(a, [0, 1]).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3558921769"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(0.3426 - 0.927, 2) + pow(0.7484-0.62853, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "# Create some variables.\n",
    "pathsave = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough'\n",
    "\n",
    "v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "inc_v1 = v1.assign(v1+1)\n",
    "dec_v2 = v2.assign(v2-1)\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    # Do some work with the model.\n",
    "    inc_v1.op.run()\n",
    "    dec_v2.op.run()\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, pathsave+\"/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough/model.ckpt\n",
      "Model restored.\n",
      "v1 : [ 1.  1.  1.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, pathsave+\"/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # Check the values of the variables\n",
    "    print(\"v1 : %s\" % v1.eval())\n",
    "    print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
