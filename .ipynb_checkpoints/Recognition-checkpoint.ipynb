{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "from DataPrep.data_io import DataFormatter\n",
    "from nn.load_params import layer_name, convShape, getWeights\n",
    "from nn.utils import getTriplets, tripletLoss\n",
    "# from train_test.model import initNetwork\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, filename=\"logfile.log\", filemode=\"w\",\n",
    "                    format=\"%(asctime)-15s %(levelname)-8s %(message)s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET INCEPTION WEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w', 'm', 'v', 'b'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moduleWeightDict['inception_3c_5x5_bn2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "for i in moduleWeightDict['dense'].keys():\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZE NETWORK WITHWEIGHTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weightDict = moduleWeightDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD THE BATCH DATA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO's:\n",
    "\n",
    "1. Remove the random weight initialiazer for the last layer, and initialize it \n",
    "   with the inception net weights.\n",
    "\n",
    "2. implement a module to save weights as checkpoints to the disk. \n",
    "\n",
    "3. create a function to toggle between Random weight initializer, Inception net weight initializer \n",
    "   and using the saved checkpoint for the last Inception layer.\n",
    "   \n",
    "4. Add more images.\n",
    "\n",
    "5. Create a complete workflow train the network and see\n",
    "\n",
    "6. Store 1 image encodings for the 3-4 labels you have.\n",
    "\n",
    "7. For a new image, pass the image throught network, get the encoding and see which is the most closest face using the encoding from the step 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "where_to_dump_model = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/classification_model'\n",
    "\n",
    "class SVM():\n",
    "    '''\n",
    "    # The embeddings in a nutshell are features. The face image goes through a complex network and results in\n",
    "    # embeddings that captures complex features of a face. SVM's are good at classifying small datasets.\n",
    "    # SVM are also robust to over fitting. The idea here is that we would wanna learn a SVM classifier using the\n",
    "    # embeddings as the feature space and see for the given embedding, how many times we are able to predict the\n",
    "    # correct class\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, embeddings, labels, iter_num=None):\n",
    "        '''\n",
    "        :param embeddings:   Embeddings of the image\n",
    "        :param labels:       labels\n",
    "        :return:\n",
    "        '''\n",
    "        model = SVC(kernel='linear', probability=True)\n",
    "        model.fit(embeddings, labels)\n",
    "        joblib.dump(model, os.path.join(where_to_dump_model, iter_num+\"_svm.sav\"))\n",
    "        \n",
    "    def classify(self, embeddings, iter_num=None):\n",
    "        '''\n",
    "        :param embeddings: Image embeddings to classify\n",
    "        :param iter_num:\n",
    "        :return:\n",
    "        '''\n",
    "        model = joblib.load(os.path.join(where_to_dump_model, iter_num+\"_svm.sav\"))\n",
    "        predLabels = model.predict_proba(embeddings)\n",
    "        best_label_idx = np.argmax(predLabels, axis=1)\n",
    "        print (predLabels)\n",
    "        print (best_label_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of input data (X) is:  (10, 18, 96, 96, 3)\n",
      "The shape of input data (Y) is:  (10, 18)\n",
      "Unique labels in dataY is:  [ 0.  1.  2.]\n",
      "Label dict:  None\n",
      "(10, 18, 96, 96, 3) (10, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/envs/anaconda35/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:96: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': <tf.Tensor 'TripletLoss/Sum_2:0' shape=<unknown> dtype=float32>, 'inpTensor': <tf.Tensor 'Placeholder:0' shape=(?, 96, 96, 3) dtype=float32>, 'optimizer': <tf.Operation 'Adam' type=NoOp>, 'output': <tf.Tensor 'InceptionFC_FT/L2_norm:0' shape=(?, 128) dtype=float32>}\n",
      "@@@@\n",
      "7.90146\n",
      "#########################\n",
      "@@@@\n",
      "9.3184\n",
      "#########################\n",
      "INFO:tensorflow:Restoring parameters from /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/saver_checkpoints/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "\n",
    "path_to_batch_data = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/batch_img_arr'\n",
    "batch_file_name = 'random_batches.pickle'\n",
    "checkpoint_path = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/saver_checkpoints'\n",
    "\n",
    "dataX, dataY, labelDict = DataFormatter.getPickleFile(folderPath=path_to_batch_data, \n",
    "                                                      picklefileName=batch_file_name, getStats=True)\n",
    "\n",
    "'''\n",
    "dataX = [num_batches, image_per_batch, image_x, image_y, image_channels]\n",
    "dataY = [num_batches, labels]\n",
    "\n",
    "'''\n",
    "print (dataX.shape, dataY.shape)\n",
    "reset_graph()\n",
    "train_encodingDict = trainEmbeddings(moduleWeightDict,init_wght_type='random')\n",
    "# add ops to save and restore model\n",
    "saver = tf.train.Saver()\n",
    "print (train_encodingDict)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    checkpoints = [ck for ck in os.listdir(checkpoint_path) if ck!='.DS_Store']\n",
    "    if len(checkpoints) > 0 :\n",
    "        saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "    for batchNum, batchX in enumerate(dataX[0:len(dataX)-8,:]):\n",
    "        batchY = dataY[batchNum, :]\n",
    "        opt, batch_loss = sess.run([train_encodingDict['optimizer'], \n",
    "                                   train_encodingDict['loss']], \n",
    "                                  feed_dict={train_encodingDict['inpTensor']:batchX})\n",
    "#         print (opt)\n",
    "        print('@@@@')\n",
    "        print (batch_loss)\n",
    "        print ('#########################')\n",
    "    save_path = saver.save(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "\n",
    "    ########  Code for Testing the model\n",
    "    # METHOD 1: TO get weights is form of Tensors\n",
    "#     saver = tf.train.import_meta_graph(os.path.join(checkpoint_path, \"model.ckpt.meta\"))\n",
    "#     saver.restore(sess,tf.train.latest_checkpoint(checkpoint_path))\n",
    "#     graph = tf.get_default_graph()\n",
    "#     w = graph.get_tensor_by_name(\"inception_5a_3x3_conv1/w:0\")\n",
    "#     print (w.shape)\n",
    "    \n",
    "    # METHOD 2: TO get weights is form of Tensors\n",
    "    a = saver.restore(sess, os.path.join(checkpoint_path, \"model.ckpt\"))\n",
    "    trainableVars = tf.get_collection(ops.GraphKeys.TRAINABLE_VARIABLES)\n",
    "    testDict = getFineTunedEmbeddings([96,96,3], moduleWeightDict, trainableVars, sess)\n",
    "    sess.run([], feed_dict = {inpTensor=batchX[10,:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'inception_5a_3x3_conv1_wb/convWeight:0' shape=(1, 1, 1024, 96) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv1_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn1_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn1_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv2_wb/convWeight:0' shape=(3, 3, 96, 384) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_conv2_wb/convBias:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn2_bn/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_3x3_bn2_bn/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_conv_wb/convWeight:0' shape=(1, 1, 1024, 96) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_conv_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_bn_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_pool_bn_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_conv_wb/convWeight:0' shape=(1, 1, 1024, 256) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_conv_wb/convBias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_bn_bn/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5a_1x1_bn_bn/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv1_wb/convWeight:0' shape=(1, 1, 736, 96) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv1_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn1_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn1_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv2_wb/convWeight:0' shape=(3, 3, 96, 384) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_conv2_wb/convBias:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn2_bn/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_3x3_bn2_bn/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_conv_wb/convWeight:0' shape=(1, 1, 736, 96) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_conv_wb/convBias:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_bn_bn/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_pool_bn_bn/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_conv_wb/convWeight:0' shape=(1, 1, 736, 256) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_conv_wb/convBias:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_bn_bn/beta:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'inception_5b_1x1_bn_bn/gamma:0' shape=(256,) dtype=float32_ref>, <tf.Variable 'InceptionFC_FT_wb/convWeight:0' shape=(736, 128) dtype=float32_ref>, <tf.Variable 'InceptionFC_FT_wb/convBias:0' shape=(128,) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "print ([v for v in tf.trainable_variables()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "tfdata = tf.cast(np.random.rand(1,1,3,5) + 10, dtype=tf.float32)\n",
    "print (tfdata.get_shape().as_list())\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batchMean, batchVar = tf.nn.moments(tfdata, axes=[0,1,2], name=\"moments\")\n",
    "    print (tfdata.eval())\n",
    "    print (batchMean.eval())\n",
    "    print (batchMean.get_shape().as_list())\n",
    "    print (batchVar.eval())\n",
    "    print (batchVar.get_shape().as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2, 3)\n",
      "(3, 3)\n",
      "[[1 2 3]\n",
      " [3 2 4]]\n"
     ]
    }
   ],
   "source": [
    "t = tf.constant([[[1, 1, 1], [2, 2, 2]],\n",
    "                 [[3, 3, 3], [4, 4, 4]],\n",
    "                 [[5, 5, 5], [6, 6, 6]]])\n",
    "\n",
    "a = tf.constant([[1,2,3],[3,2,4],[1,1,4]])\n",
    "print(t.shape)\n",
    "print (a.shape)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print (tf.gather(a, [0, 1]).eval())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3558921769"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(0.3426 - 0.927, 2) + pow(0.7484-0.62853, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "\n",
    "# Create some variables.\n",
    "pathsave = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough'\n",
    "\n",
    "v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "inc_v1 = v1.assign(v1+1)\n",
    "dec_v2 = v2.assign(v2-1)\n",
    "\n",
    "# Add an op to initialize the variables.\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, initialize the variables, do some work, and save the\n",
    "# variables to disk.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    # Do some work with the model.\n",
    "    inc_v1.op.run()\n",
    "    dec_v2.op.run()\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(sess, pathsave+\"/model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /Users/sam/All-Program/App-DataSet/DeepFaceRecognition/data_models/rough/model.ckpt\n",
      "Model restored.\n",
      "v1 : [ 1.  1.  1.]\n",
      "v2 : [-1. -1. -1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Create some variables.\n",
    "v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, pathsave+\"/model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # Check the values of the variables\n",
    "    print(\"v1 : %s\" % v1.eval())\n",
    "    print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
