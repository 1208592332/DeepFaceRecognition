{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "picPath = '/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceRecognition/camera_0.jpg'\n",
    "\n",
    "from scipy import misc\n",
    "\n",
    "def readImage(imagePath):\n",
    "    '''\n",
    "        The input data is is in the shape of [nh, nw, nc], convert it to [nc, nh, nw]\n",
    "    '''\n",
    "    image = misc.imread(picPath)\n",
    "    \n",
    "    img = np.around(np.transpose(image, (2,0,1))/255.0, decimals=12) #(2,0,1) = [nc, nh, nw]\n",
    "    print (image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from numpy import genfromtxt\n",
    "from nn.LoadParams import layer_name, convShape, getWeights\n",
    "# from keras import backend as K\n",
    "# from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "# from keras.models import Model\n",
    "# from keras.layers.normalization import BatchNormalization\n",
    "# from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "# from keras.layers.core import Lambda, Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1\n",
      "(7, 7, 3, 64)\n",
      "(64,)\n",
      "bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv2\n",
      "(1, 1, 64, 64)\n",
      "(64,)\n",
      "bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "conv3\n",
      "(3, 3, 64, 192)\n",
      "(192,)\n",
      "bn3\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_3a_1x1_conv\n",
      "(1, 1, 192, 64)\n",
      "(64,)\n",
      "inception_3a_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3a_5x5_conv1\n",
      "(1, 1, 192, 16)\n",
      "(16,)\n",
      "inception_3a_5x5_conv2\n",
      "(5, 5, 16, 32)\n",
      "(32,)\n",
      "inception_3a_5x5_bn1\n",
      "(16,) (16,) (16,) (16,)\n",
      "inception_3a_5x5_bn2\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3a_3x3_conv1\n",
      "(1, 1, 192, 96)\n",
      "(96,)\n",
      "inception_3a_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3a_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3a_pool_conv\n",
      "(1, 1, 192, 32)\n",
      "(32,)\n",
      "inception_3a_pool_bn\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_1x1_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_1x1_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_3x3_conv1\n",
      "(1, 1, 256, 96)\n",
      "(96,)\n",
      "inception_3b_3x3_conv2\n",
      "(3, 3, 96, 128)\n",
      "(128,)\n",
      "inception_3b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_3b_3x3_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3b_5x5_conv1\n",
      "(1, 1, 256, 32)\n",
      "(32,)\n",
      "inception_3b_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3b_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3b_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3b_pool_conv\n",
      "(1, 1, 256, 64)\n",
      "(64,)\n",
      "inception_3b_pool_bn\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_3c_3x3_conv1\n",
      "(1, 1, 320, 128)\n",
      "(128,)\n",
      "inception_3c_3x3_conv2\n",
      "(3, 3, 128, 256)\n",
      "(256,)\n",
      "inception_3c_3x3_bn1\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_3c_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_3c_5x5_conv1\n",
      "(1, 1, 320, 32)\n",
      "(32,)\n",
      "inception_3c_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_3c_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_3c_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_1x1_conv\n",
      "(1, 1, 640, 256)\n",
      "(256,)\n",
      "inception_4a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4a_3x3_conv1\n",
      "(1, 1, 640, 96)\n",
      "(96,)\n",
      "inception_4a_3x3_conv2\n",
      "(3, 3, 96, 192)\n",
      "(192,)\n",
      "inception_4a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_4a_3x3_bn2\n",
      "(192,) (192,) (192,) (192,)\n",
      "inception_4a_5x5_conv1\n",
      "(1, 1, 640, 32)\n",
      "(32,)\n",
      "inception_4a_5x5_conv2\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "inception_4a_5x5_bn1\n",
      "(32,) (32,) (32,) (32,)\n",
      "inception_4a_5x5_bn2\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4a_pool_conv\n",
      "(1, 1, 640, 128)\n",
      "(128,)\n",
      "inception_4a_pool_bn\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_4e_3x3_conv1\n",
      "(1, 1, 640, 160)\n",
      "(160,)\n",
      "inception_4e_3x3_conv2\n",
      "(3, 3, 160, 256)\n",
      "(256,)\n",
      "inception_4e_3x3_bn1\n",
      "(160,) (160,) (160,) (160,)\n",
      "inception_4e_3x3_bn2\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_4e_5x5_conv1\n",
      "(1, 1, 640, 64)\n",
      "(64,)\n",
      "inception_4e_5x5_conv2\n",
      "(5, 5, 64, 128)\n",
      "(128,)\n",
      "inception_4e_5x5_bn1\n",
      "(64,) (64,) (64,) (64,)\n",
      "inception_4e_5x5_bn2\n",
      "(128,) (128,) (128,) (128,)\n",
      "inception_5a_1x1_conv\n",
      "(1, 1, 1024, 256)\n",
      "(256,)\n",
      "inception_5a_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5a_3x3_conv1\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5a_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5a_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5a_pool_conv\n",
      "(1, 1, 1024, 96)\n",
      "(96,)\n",
      "inception_5a_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_1x1_conv\n",
      "(1, 1, 736, 256)\n",
      "(256,)\n",
      "inception_5b_1x1_bn\n",
      "(256,) (256,) (256,) (256,)\n",
      "inception_5b_3x3_conv1\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_3x3_conv2\n",
      "(3, 3, 96, 384)\n",
      "(384,)\n",
      "inception_5b_3x3_bn1\n",
      "(96,) (96,) (96,) (96,)\n",
      "inception_5b_3x3_bn2\n",
      "(384,) (384,) (384,) (384,)\n",
      "inception_5b_pool_conv\n",
      "(1, 1, 736, 96)\n",
      "(96,)\n",
      "inception_5b_pool_bn\n",
      "(96,) (96,) (96,) (96,)\n"
     ]
    }
   ],
   "source": [
    "parentPath = \"/Users/sam/All-Program/App-DataSet/Deep-Neural-Nets/Models/FaceNet-Inception\"\n",
    "moduleWeightDict = getWeights(parentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'jetha', '2': 'raj', '0': 'sam'}\n"
     ]
    }
   ],
   "source": [
    "from nn.Network import getModel\n",
    "\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "# def getData(dataPath):\n",
    "#     with open()\n",
    "    \n",
    "# img = readImage(picPath)\n",
    "dataPath = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/full_data_ndarr.pickle\"\n",
    "with open(dataPath, \"rb\") as p:\n",
    "    data = pickle.load(p)\n",
    "    dataX = data['dataX']\n",
    "    dataY = data['dataY']\n",
    "    labelDict = data['labelDict']\n",
    "    print (labelDict)\n",
    "    \n",
    "# img = np.reshape(img, (1,img.shape[0], img.shape[1], img.shape[2]))\n",
    "# print (img.shape)\n",
    "# reset_graph()\n",
    "# tensorDict = getModel(img.shape, params=moduleWeightDict)\n",
    "    #(img, conv1_w, conv1_b, s=2, pad='SAME',  scope_name='conv1', isTrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 96, 3)\n",
      "inpTensor  (?, 96, 96, 3)\n",
      "conv1:  (?, 48, 48, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 50, 50, 64)\n",
      "conv1 Zero-Padding + MAXPOOL  (?, 24, 24, 64)\n",
      "conv2:  (?, 24, 24, 64)\n",
      "conv2 Zero-Padding + MAXPOOL  (?, 26, 26, 64)\n",
      "conv3:  (?, 24, 24, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 26, 26, 192)\n",
      "conv3 Zero-Padding + MAXPOOL  (?, 12, 12, 192)\n",
      "Inside Inception module1:  (?, 12, 12, 192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sam/App-Setup/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain 2:  (?, 12, 12, 128)\n",
      "Chain 3:  (?, 12, 12, 32)\n",
      "Chain 4:  (?, 12, 12, 32)\n",
      "Chain 1:  (?, 12, 12, 64)\n",
      "inception3a:  (?, 12, 12, 256)\n",
      "Inside Inception module1:  (?, 12, 12, 256)\n",
      "Chain 2:  (?, 12, 12, 128)\n",
      "Chain 3:  (?, 12, 12, 64)\n",
      "Chain 4:  (?, 12, 12, 64)\n",
      "Chain 1:  (?, 12, 12, 64)\n",
      "inception3b:  (?, 12, 12, 320)\n"
     ]
    }
   ],
   "source": [
    "from nn.Inception import getModel\n",
    "\n",
    "def reset_graph():  # Reset the graph\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "img = readImage(picPath)\n",
    "# img = np.reshape(img, (1,img.shape[0], img.shape[1], img.shape[2]))\n",
    "img.shape\n",
    "reset_graph()\n",
    "tensorDict = getModel(img.shape, params=moduleWeightDict)#(img, conv1_w, conv1_b, s=2, pad='SAME',  scope_name='conv1', isTrainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print (img.shape)\n",
    "    img1 = np.reshape(img, (1,img.shape[0], img.shape[1], img.shape[2]))\n",
    "    print (img1.shape)\n",
    "    x = sess.run([tensorDict['output']], feed_dict={tensorDict['inpTensor']:img1})\n",
    "    print (x[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.ops.data_flow_ops.FIFOQueue object at 0x12e6d5f98>\n",
      "Tensor(\"DecodeJpeg_1:0\", shape=(?, ?, ?), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "imagePath = '/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/'\n",
    "filename_queue = tf.train.string_input_producer(\n",
    "    tf.train.match_filenames_once(imagePath+'/*.jpg'))\n",
    "\n",
    "print (filename_queue)\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "_, image_file = image_reader.read(filename_queue)\n",
    "\n",
    "image = tf.image.decode_jpeg(image_file)\n",
    "print (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "parent_path = \"/Users/sam/All-Program/App-DataSet/DeepFaceRecognition/\"\n",
    "\n",
    "\n",
    "\n",
    "class DataPrep():\n",
    "    def __init__ (self, parentPath):\n",
    "        self.parentPath = parentPath\n",
    "        self.resized_image_path = os.path.join(self.parentPath, 'resized')\n",
    "        \n",
    "    def imageToArray(self):\n",
    "        if not os.path.exists(self.resized_image_path):\n",
    "            raise ValueError('You should run the resize image dump before running imageToArray')\n",
    "            \n",
    "        person_names = [person_name for person_name in os.listdir(self.resized_image_path) \n",
    "                        if person_name != \".DS_Store\"]\n",
    "        dataX = []\n",
    "        dataY = []\n",
    "        labelDict = {}\n",
    "        for label, person_name in enumerate(person_names):\n",
    "            photo_path = os.path.join(self.resized_image_path, person_name)\n",
    "            person_pics = [os.path.join(photo_path, pics) for pics in os.listdir(photo_path) if pics.split('.')[1] == \"jpg\"]\n",
    "            labelDict[str(label)] = person_name\n",
    "            per_person_img_arr = np.ndarray((len(person_pics), 96,96,3))\n",
    "            per_person_labels = np.tile(label, len(person_pics)).reshape(-1,1)\n",
    "            for img_num, pic_path in enumerate(person_pics):\n",
    "                image = ndimage.imread(pic_path, mode='RGB')\n",
    "                per_person_img_arr[img_num,:] = image\n",
    "            if label == 0:\n",
    "                dataX = per_person_img_arr\n",
    "                dataY = per_person_labels\n",
    "            else:\n",
    "                dataX = np.vstack((dataX, per_person_img_arr))\n",
    "                dataY = np.vstack((dataY, per_person_labels))\n",
    "        return dataX, dataY, labelDict\n",
    "\n",
    "    def dumpImageArr(self, dataX, dataY, labelDict=None):\n",
    "\n",
    "        with open(os.path.join(self.parentPath, 'full_data_ndarr.pickle'), 'wb') as f:\n",
    "            fullData = {\n",
    "                        'dataX': dataX,\n",
    "                        'dataY': dataY,\n",
    "                        'labelDict': labelDict\n",
    "                        }\n",
    "            pickle.dump(fullData, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "objDP = DataPrep(parent_path)\n",
    "dataX, dataY, labelDict = objDP.imageToArray()\n",
    "objDP.dumpImageArr(dataX, dataY, labelDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
